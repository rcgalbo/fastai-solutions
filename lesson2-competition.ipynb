{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys; sys.path.append('../courses/deeplearning1/nbs/')\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if keras giving hard time\n",
    "# !pip install --upgrade keras --user\n",
    "# !pip install --upgrade tensorflow --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do:\n",
    "1. load into data frame\n",
    "2. convert to numpy array\n",
    "3. average images to make 3rd band\n",
    "4. stack to array shape(#_ims, 75,75,3)\n",
    "5. make train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_json('data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1604 entries, 0 to 1603\n",
      "Data columns (total 5 columns):\n",
      "band_1        1604 non-null object\n",
      "band_2        1604 non-null object\n",
      "id            1604 non-null object\n",
      "inc_angle     1604 non-null object\n",
      "is_iceberg    1604 non-null int64\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 75.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band_1</th>\n",
       "      <th>band_2</th>\n",
       "      <th>id</th>\n",
       "      <th>inc_angle</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-27.878361, -27.15416, -28.668615, -29.537971...</td>\n",
       "      <td>[-27.154118, -29.537888, -31.0306, -32.190483,...</td>\n",
       "      <td>dfd5f913</td>\n",
       "      <td>43.9239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-12.242375, -14.920305, -14.920363, -12.66633...</td>\n",
       "      <td>[-31.506321, -27.984554, -26.645678, -23.76760...</td>\n",
       "      <td>e25388fd</td>\n",
       "      <td>38.1562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-24.603676, -24.603714, -24.871029, -23.15277...</td>\n",
       "      <td>[-24.870956, -24.092632, -20.653963, -19.41104...</td>\n",
       "      <td>58b2aaa0</td>\n",
       "      <td>45.2859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-22.454607, -23.082819, -23.998013, -23.99805...</td>\n",
       "      <td>[-27.889421, -27.519794, -27.165262, -29.10350...</td>\n",
       "      <td>4cfc3a18</td>\n",
       "      <td>43.8306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-26.006956, -23.164886, -23.164886, -26.89116...</td>\n",
       "      <td>[-27.206915, -30.259186, -30.259186, -23.16495...</td>\n",
       "      <td>271f93f4</td>\n",
       "      <td>35.6256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              band_1  \\\n",
       "0  [-27.878361, -27.15416, -28.668615, -29.537971...   \n",
       "1  [-12.242375, -14.920305, -14.920363, -12.66633...   \n",
       "2  [-24.603676, -24.603714, -24.871029, -23.15277...   \n",
       "3  [-22.454607, -23.082819, -23.998013, -23.99805...   \n",
       "4  [-26.006956, -23.164886, -23.164886, -26.89116...   \n",
       "\n",
       "                                              band_2        id inc_angle  \\\n",
       "0  [-27.154118, -29.537888, -31.0306, -32.190483,...  dfd5f913   43.9239   \n",
       "1  [-31.506321, -27.984554, -26.645678, -23.76760...  e25388fd   38.1562   \n",
       "2  [-24.870956, -24.092632, -20.653963, -19.41104...  58b2aaa0   45.2859   \n",
       "3  [-27.889421, -27.519794, -27.165262, -29.10350...  4cfc3a18   43.8306   \n",
       "4  [-27.206915, -30.259186, -30.259186, -23.16495...  271f93f4   35.6256   \n",
       "\n",
       "   is_iceberg  \n",
       "0           0  \n",
       "1           0  \n",
       "2           1  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert images to numpy array\n",
    "def img_to_array(img_list): \n",
    "    return np.asanyarray(img_list, dtype=np.float32).reshape((75,75))\n",
    "train.band_1 = train.band_1.apply(img_to_array)\n",
    "train.band_2 = train.band_2.apply(img_to_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# create one_hot response\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y_tr = to_categorical(train.is_iceberg.values,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning globals -> for fast iter (need to be optimized)\n",
    "tr_len = train.shape[0]\n",
    "nb_classes = len(train.is_iceberg.unique())\n",
    "batch_size = 16\n",
    "epoch = 500\n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make channel # before dims -> theano dim ordering\n",
    "# create training set of (train_len, 3, 75, 75)\n",
    "ch_1 = np.vstack(train.band_1.values).reshape(tr_len,75,75)\n",
    "ch_2 = np.vstack(train.band_1.values).reshape(tr_len,75,75)\n",
    "# add 3rd channel of average input\n",
    "ch_3 = ((ch_1 + ch_2) / 2)\n",
    "\n",
    "X_tr = np.stack((ch_1,ch_2,ch_3),axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create validation set for train and test\n",
    "import random; ind = random.sample(range(tr_len), 300)\n",
    "\n",
    "X_val = X_tr[ind]\n",
    "Y_val = Y_tr[ind]\n",
    "\n",
    "# make sure to remove the validation samples from the training set\n",
    "X_tr = np.delete(X_tr,ind,0)\n",
    "Y_tr = np.delete(Y_tr,ind,0)\n",
    "\n",
    "# # reshape for theano dim leave for tf backend\n",
    "# X_tr = np.reshape(X_tr,(X_tr.shape[0],3,75,75))\n",
    "# # reshape for theano dim leave for tf backend\n",
    "# X_val = np.reshape(X_val,(X_val.shape[0],3,75,75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1304, 75, 75, 3), (1304, 2), (300, 75, 75, 3), (300, 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: x.shape, [X_tr,Y_tr,X_val,Y_val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load resnet from keras\n",
    "- iterate through layers to find best representation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use functional api to work with applications\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "\n",
    "def FCBlock(x):\n",
    "    x = Dense(4096, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    return x\n",
    "\n",
    "def build_model():\n",
    "    # get base model from \n",
    "    # change input shape based upon backend, tf == dim first\n",
    "    inp = Input(shape=(75,75,3))\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_tensor=inp)\n",
    "    \n",
    "    # make base model untrainable -> will pass through because of functional api\n",
    "    for layer in base_model.layers: layer.trainable=False\n",
    "    \n",
    "    x = base_model.output\n",
    "    \n",
    "    # flatten last conv layer\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # add fully connected layers\n",
    "    for i in range(2): x = FCBlock(x)\n",
    "    \n",
    "    # add fully connected output\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    return Model(input=base_model.input, output=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vgg-structure](data/vgg16.png)\n",
    "\n",
    ">using this network structure but instead of 1000 classes we're using is_iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # compile model to train last layers\n",
    "# model = build_model()\n",
    "# model.compile(optimizer=SGD(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # fit model on sample data\n",
    "# checkpointer = ModelCheckpoint(filepath='data/model/weights.hdf5',verbose=1, save_best_only=True)\n",
    "# early_stop = EarlyStopping(monitor='val_loss', patience = 3)\n",
    "# model.fit(X_tr, Y_tr,\n",
    "#           batch_size=batch_size,\n",
    "#           nb_epoch=epoch, \n",
    "#           validation_data=(X_val,Y_val), \n",
    "#           callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model from scratch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#Building the model\n",
    "def build_model():\n",
    "    model=Sequential()\n",
    "\n",
    "    #Conv Layer 1\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(75, 75, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the most recent weights\n",
    "model.load_weights('data/model/weights_custom_model1_run1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1304 samples, validate on 300 samples\n",
      "Epoch 1/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4983 - acc: 0.7230Epoch 00001: val_loss improved from inf to 0.52942, saving model to data/model/weights_custom_model1_run2.hdf5\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.4989 - acc: 0.7224 - val_loss: 0.5294 - val_acc: 0.6633\n",
      "Epoch 2/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.5003 - acc: 0.7338Epoch 00002: val_loss improved from 0.52942 to 0.48034, saving model to data/model/weights_custom_model1_run2.hdf5\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.5003 - acc: 0.7331 - val_loss: 0.4803 - val_acc: 0.7600\n",
      "Epoch 3/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4845 - acc: 0.7492Epoch 00003: val_loss did not improve\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.4851 - acc: 0.7485 - val_loss: 0.4916 - val_acc: 0.7200\n",
      "Epoch 4/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4827 - acc: 0.7431Epoch 00004: val_loss did not improve\n",
      "1304/1304 [==============================] - 23s 18ms/step - loss: 0.4853 - acc: 0.7400 - val_loss: 0.4908 - val_acc: 0.7267\n",
      "Epoch 5/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.7299Epoch 00005: val_loss improved from 0.48034 to 0.47811, saving model to data/model/weights_custom_model1_run2.hdf5\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.4868 - acc: 0.7308 - val_loss: 0.4781 - val_acc: 0.7567\n",
      "Epoch 6/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4841 - acc: 0.7276Epoch 00006: val_loss did not improve\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.4851 - acc: 0.7262 - val_loss: 0.4818 - val_acc: 0.7400\n",
      "Epoch 7/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4539 - acc: 0.7554Epoch 00007: val_loss did not improve\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.4548 - acc: 0.7554 - val_loss: 0.5130 - val_acc: 0.7000\n",
      "Epoch 8/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4579 - acc: 0.7600Epoch 00008: val_loss improved from 0.47811 to 0.45726, saving model to data/model/weights_custom_model1_run2.hdf5\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.4573 - acc: 0.7615 - val_loss: 0.4573 - val_acc: 0.7600\n",
      "Epoch 9/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.7631Epoch 00009: val_loss did not improve\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.4555 - acc: 0.7646 - val_loss: 0.4612 - val_acc: 0.7567\n",
      "Epoch 10/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4571 - acc: 0.7492Epoch 00010: val_loss did not improve\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.4595 - acc: 0.7477 - val_loss: 0.5105 - val_acc: 0.7000\n",
      "Epoch 11/100\n",
      "1296/1304 [============================>.] - ETA: 0s - loss: 0.4698 - acc: 0.7531Epoch 00011: val_loss did not improve\n",
      "1304/1304 [==============================] - 24s 18ms/step - loss: 0.4717 - acc: 0.7515 - val_loss: 0.5041 - val_acc: 0.7033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf235b8e90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='data/model/weights_custom_model1_run2.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience = 3)\n",
    "model.fit(X_tr,Y_tr,\n",
    "          batch_size=12,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, Y_val),\n",
    "         callbacks=[checkpointer, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load test data and resize for predictions\n",
    "test = pd.read_json('data/test.json')\n",
    "\n",
    "# conver to array\n",
    "test.band_1 = test.band_1.apply(img_to_array)\n",
    "test.band_2 = test.band_2.apply(img_to_array)\n",
    "# make channel # before dims -> theano dim ordering\n",
    "# create training set of (train_len, 3, 75, 75)\n",
    "tst_len = test.shape[0]\n",
    "ch_1 = np.vstack(test.band_1.values).reshape(tst_len,75,75)\n",
    "ch_2 = np.vstack(test.band_1.values).reshape(tst_len,75,75)\n",
    "# add 3rd channel of average input\n",
    "ch_3 = ((ch_1 + ch_2) / 2)\n",
    "\n",
    "X_test = np.stack((ch_1,ch_2,ch_3),axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose model weights and create predictions \n",
    "model.load_weights('data/model/weights_custom_model1_run2.hdf5')\n",
    "\n",
    "preds = model.predict(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_iceberg = preds[:,0]\n",
    "id = test['id'].astype(str).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = pd.DataFrame({'id':id, 'is_iceberg':is_iceberg})\n",
    "preds.to_csv('data/preds1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='data/preds1.csv' target='_blank'>data/preds1.csv</a><br>"
      ],
      "text/plain": [
       "/mnt/data/preds1.csv"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('data/preds1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
